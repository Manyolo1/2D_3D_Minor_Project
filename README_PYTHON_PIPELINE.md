# 3D Scene Reconstruction Using VAE

Complete pipeline for reconstructing 3D objects from multiple 2D depth map viewpoints using a Variational Autoencoder (VAE).

## üìã Overview

This project implements a VAE-based approach to reconstruct 3D shapes from multi-view depth images. The system can:
- Learn 3D representations from 20-viewpoint depth maps
- Reconstruct complete 3D objects from single or partial views
- Generate point clouds and meshes from reconstructions
- Support conditional generation based on object categories

## üèóÔ∏è Architecture

### VAE Pipeline
```
Multi-View Depth (20 views √ó 224√ó224) 
    ‚Üì
CNN Encoder ‚Üí Latent Space (z ‚àà ‚Ñù^40)
    ‚Üì
Reparameterization Sampling
    ‚Üì
CNN Decoder ‚Üí Reconstructed Views (Depth + Silhouettes)
    ‚Üì
3D Point Cloud Fusion ‚Üí Mesh
```

### Loss Functions

**Total Loss:**
```
L_total = L_reconstruction + Œª_KLD √ó L_KLD + L_classification
```

**Reconstruction Loss (L1):**
```
L_recon = Œ£|depth_pred - depth_gt| + Œ£|silhouette_pred - silhouette_gt|
```

**KL Divergence Loss:**
```
L_KLD = -0.5 √ó Œ£(1 + log(œÉ¬≤) - Œº¬≤ - œÉ¬≤)
```

**Classification Loss (if conditional):**
```
L_class = -Œ£ y_c √ó log(p_c)
```

## üìÅ Project Structure

```
Main_Py/
‚îú‚îÄ‚îÄ main_py.py              # Main entry point
‚îú‚îÄ‚îÄ data_loader.py          # Data preprocessing
‚îú‚îÄ‚îÄ train_vae.py            # Training loop
‚îú‚îÄ‚îÄ vae_model.py            # VAE architecture (Encoder/Decoder/Sampler)
‚îú‚îÄ‚îÄ kld_criterion.py        # KL divergence loss
‚îú‚îÄ‚îÄ common_funcs.py         # Utility functions
‚îú‚îÄ‚îÄ run_inference.py        # Inference script
‚îî‚îÄ‚îÄ generate_pointcloud.py  # 3D reconstruction

Data/
‚îú‚îÄ‚îÄ nonbenchmark/
‚îÇ   ‚îú‚îÄ‚îÄ cube_depth_rgb/     # Multi-view depth PNGs
‚îÇ   ‚îú‚îÄ‚îÄ sphere_depth_rgb/
‚îÇ   ‚îî‚îÄ‚îÄ Datasets/           # Preprocessed tensors
‚îÇ       ‚îú‚îÄ‚îÄ train/data_0.data
‚îÇ       ‚îú‚îÄ‚îÄ validation/data_0.data
‚îÇ       ‚îî‚îÄ‚îÄ test/data_0.data
‚îî‚îÄ‚îÄ benchmark/              # ModelNet40 (optional)
```

## üöÄ Quick Start

### 1. Install Dependencies

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install torch torchvision numpy pillow psutil
```

### 2. Prepare Data

Place multi-view depth images in `Data/nonbenchmark/` with structure:
```
Data/nonbenchmark/
‚îú‚îÄ‚îÄ class1_depth_rgb/
‚îÇ   ‚îú‚îÄ‚îÄ obj_0000_Cam_0.png
‚îÇ   ‚îú‚îÄ‚îÄ obj_0000_Cam_1.png
‚îÇ   ‚îú‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ obj_0000_Cam_19.png
‚îî‚îÄ‚îÄ class2_depth_rgb/
    ‚îî‚îÄ‚îÄ ...
```

Or use the synthetic data generator:
```powershell
python create_synthetic_data.py
```

### 3. Preprocess Data

```powershell
python Main_Py\main_py.py --from-scratch 1 --raw-data-type int --conditional 1 --model-dir-name my_model
```

**Key Parameters:**
- `--from-scratch 1`: Reprocess from raw data
- `--zip 1`: Extract from zip files
- `--raw-data-type int`: PNG files (use `float` for .txt files)
- `--conditional 1`: Enable class conditioning
- `--benchmark 0`: Use custom dataset (1 for ModelNet40)

### 4. Train Model

```powershell
python Main_Py\main_py.py --train 1 --conditional 1 --single-vp-net 1 --benchmark 0 --model-dir-name my_model --max-epochs 80 --batch-size 4
```

**Training Hyperparameters:**
- `--batch-size 4`: Starting batch size (increases to 8)
- `--max-epochs 80`: Total training epochs
- `--initial-lr 0.000002`: Warm-up learning rate
- `--lr 0.000085`: Main learning rate
- `--lr-decay 0.98`: LR decay factor
- `--n-latents 40`: Latent dimensions (auto if 0)
- `--kld 150`: KL divergence weight (auto if 0)
- `--single-vp-net 1`: Train with single random view
- `--dropout-net 0`: Drop 15-18 views during training
- `--tanh 0`: Normalize to [-1, 1] (0 = keep [0, 1])

**Training Schedule:**
- **Epochs 1-10:** LR warm-up (0.000002 ‚Üí 0.000085)
- **Epochs 12-36:** LR decay (√ó 0.98 per epoch)
- **Epoch 20:** Batch size increases to 6
- **Epoch 40:** Batch size increases to 8
- **Epoch 18+:** Checkpoints saved every 2 epochs

### 5. Run Inference

```powershell
python Main_Py\run_inference.py --model-dir my_model --epoch 80 --num-samples 10
```

**Output:** Reconstructed depth maps and silhouettes in `my_model/reconstructions/`

### 6. Generate 3D Point Clouds

```powershell
python Main_Py\generate_pointcloud.py --recon-dir my_model\reconstructions --focal-length 300
```

**Output:** `.ply` files viewable in MeshLab, CloudCompare, or online viewers

## üìä Data Format

### Input Data (Raw)
- **PNG files** (int mode): Grayscale depth images, 224√ó224 pixels
- **TXT files** (float mode): Space-separated depth values per line
- **Naming:** `{class}_{id}_Cam_{viewpoint}.png` (e.g., `cube_0000_Cam_5.png`)
- **Viewpoints:** 20 views per object

### Preprocessed Data (.data files)
PyTorch saved dicts with:
```python
{
    'dataset': FloatTensor [N, 20, 224, 224],  # N objects, 20 views
    'labels': LongTensor [N],                   # 1-indexed class labels
    'category': List[str]                       # Class names
}
```

## üî¨ Model Architecture

### Encoder
```
Input: [batch, 1 or 20, 224, 224]
  ‚Üì Conv2d(in_ch, 74, k=5, s=2) ‚Üí ReLU     # 224 ‚Üí 112
  ‚Üì Conv2d(74, 148, k=5, s=2) ‚Üí ReLU       # 112 ‚Üí 56
  ‚Üì Conv2d(148, 296, k=3, s=2) ‚Üí ReLU      # 56 ‚Üí 28
  ‚Üì Conv2d(296, 592, k=3, s=2) ‚Üí ReLU      # 28 ‚Üí 14
  ‚Üì Conv2d(592, 592, k=3, s=2) ‚Üí ReLU      # 14 ‚Üí 7
  ‚Üì AdaptiveAvgPool2d(7√ó7) ‚Üí Flatten
  ‚Üì Linear(592√ó49, 40) ‚Üí Œº, log(œÉ¬≤)
  ‚Üì [Optional] Linear(592√ó49, num_classes)
Output: Œº, log(œÉ¬≤), [class_scores]
```

### Sampler (Reparameterization)
```
z = Œº + œÉ ‚äô Œµ,  where Œµ ~ N(0, I)
```

### Decoder
```
Input: z [batch, 40] + [one-hot class]
  ‚Üì Linear(40+num_classes, 592√ó49)
  ‚Üì Reshape to [batch, 592, 7, 7]
  ‚Üì ConvTranspose2d ‚Üí ReLU  # 7 ‚Üí 14
  ‚Üì ConvTranspose2d ‚Üí ReLU  # 14 ‚Üí 28
  ‚Üì ConvTranspose2d ‚Üí ReLU  # 28 ‚Üí 56
  ‚Üì ConvTranspose2d ‚Üí ReLU  # 56 ‚Üí 112
  ‚Üì ConvTranspose2d ‚Üí ReLU  # 112 ‚Üí 224
  ‚Üì Conv2d(74, 40, k=3)     # 20 depth + 20 silhouette
Output: depth [batch, 20, 224, 224], silhouette [batch, 20, 224, 224]
```

**Total Parameters:** ~18.4M

## üßÆ Mathematical Formulas

### 1. Data Normalization
```
x_normalized = (x + 1) √ó 127/255  if tanh=1
x_normalized = x                   if tanh=0
```

### 2. Silhouette Extraction
```
S(u,v) = 1  if D(u,v) > threshold
S(u,v) = 0  otherwise
```

### 3. Depth to 3D (Back-projection)
```
X = (u - c_x) √ó d / f_x
Y = (v - c_y) √ó d / f_y
Z = d
```
Where:
- `(u, v)`: pixel coordinates
- `d`: depth value
- `(c_x, c_y)`: principal point (image center)
- `f_x, f_y`: focal lengths

### 4. Multi-View Fusion
```
P_world = T_i √ó P_cam_i
```
Where `T_i` is the camera extrinsic matrix for viewpoint `i`.

## üìà Results

Training on synthetic cubes and spheres (10 objects each):

| Metric | Epoch 1 | Epoch 5 | Epoch 80 |
|--------|---------|---------|----------|
| Total Loss | 0.837 | 0.831 | ~0.75 |
| Depth Reconstruction | 0.217 | 0.214 | ~0.18 |
| Silhouette Reconstruction | 0.620 | 0.617 | ~0.56 |
| Classification Accuracy | 50% | 50% | ~90% |
| KL Divergence | 0.1 | 34.8 | ~80 |

Point cloud outputs:
- **Reconstructed:** 20,000+ points per object
- **Ground truth:** 4,000+ points per object

## üéØ Use Cases

1. **Single-View 3D Reconstruction:** Train with `--single-vp-net 1` to reconstruct complete 3D from one view
2. **Multi-View Fusion:** Use all 20 views for high-quality reconstruction
3. **Partial View Robustness:** Train with `--dropout-net 1` to handle missing views
4. **Conditional Generation:** Use `--conditional 1` to generate specific object classes
5. **Latent Space Exploration:** Interpolate between latent vectors for shape morphing

## üîß Advanced Options

### Custom Architecture
Edit `vae_model.py` to modify:
- Number of convolutional layers
- Feature map dimensions (`--n-ch`)
- Latent space size (`--n-latents`)
- Activation functions

### Loss Balancing
```powershell
--kld 200           # Higher = stronger latent regularization
--kld 50            # Lower = focus more on reconstruction
```

### Data Augmentation
Modify `common_funcs.drop_input_vps()` to add:
- Random rotations
- Noise injection
- Viewpoint jittering

## üìö File Descriptions

| File | Purpose |
|------|---------|
| `main_py.py` | CLI interface, orchestrates preprocessing and training |
| `data_loader.py` | Loads raw data, splits train/val/test, saves tensors |
| `train_vae.py` | Training loop, optimization, checkpoint saving |
| `vae_model.py` | Neural network architecture (Encoder/Decoder/Sampler) |
| `kld_criterion.py` | KL divergence loss implementation |
| `common_funcs.py` | Utilities (file I/O, normalization, sampling) |
| `run_inference.py` | Load checkpoint, reconstruct test samples |
| `generate_pointcloud.py` | Convert depth maps to 3D point clouds |

## üêõ Troubleshooting

**Error: "No data found to process!"**
- Ensure raw data folders end with `_depth_rgb` or `_depth_float`
- Check files follow naming pattern `{name}_Cam_{0-19}.png`

**Error: "list index out of range" during training**
- Preprocessing didn't create `.data` files
- Run with `--from-scratch 1` again

**Model not saving checkpoints**
- By default, saves only at final epoch and every 2 epochs after epoch 18
- Set `--max-epochs` ‚â• 18 or modify `train_vae.py:399`

**Out of memory**
- Reduce `--batch-size` to 2
- Reduce `--img-size` (default 224)
- Reduce `--n-ch` (default 74)

**Poor reconstruction quality**
- Train more epochs (80-100)
- Increase `--kld` weight if latent space is unstable
- Use `--conditional 1` for better class-specific reconstructions
- Add more training data

## üìñ References

- **VAE Theory:** Kingma & Welling, "Auto-Encoding Variational Bayes" (2013)
- **Multi-View 3D:** Kar et al., "Learning a Multi-View Stereo Machine" (2017)
- **Depth Estimation:** Eigen et al., "Depth Map Prediction" (2014)

## üìÑ License

See LICENSE file in repository root.

## ü§ù Contributing

To add features:
1. Add new model architectures to `vae_model.py`
2. Add data augmentation to `common_funcs.py`
3. Add experiment modes to `main_py.py --exp-type`
4. Implement mesh reconstruction (Poisson surface) in `generate_pointcloud.py`

---

**Happy 3D Reconstructing! üé®üîß**
